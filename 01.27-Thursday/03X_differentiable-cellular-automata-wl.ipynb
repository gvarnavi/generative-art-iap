{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9869fa43",
   "metadata": {},
   "source": [
    "[![Open In Wolfram Cloud](https://raw.githubusercontent.com/gvarnavi/generative-art-iap/master/PR/wolfram-badge.svg)](https://www.wolframcloud.com/obj/gvarnavi/Published/03_differentiable-cellular-automata.nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89515e1",
   "metadata": {},
   "source": [
    "# Differentiable CAs (NNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c1fdd",
   "metadata": {},
   "source": [
    "So far, we've mostly looked at discrete state CA, e.g. our 'binary' elementary CAs - which could either be \"alive\" or \"dead\".  \n",
    "\n",
    "I recently stumbled upon this [excellent blog post](https://distill.pub/2020/growing-ca/) about a differentiable model of morphogenesis using neural networks and cellular automata, and figured it would be a great demonstration for the class!\n",
    "\n",
    "In order to allow for our neural networks to minimize the gradient loss function - we need to allow for continuum state CAs, e.g. taking any real value between 0-1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8fcd6",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "I recommend reading the blog post above after class, but essentially we're trying to teach a neural network to learn the update rules for a continuous state cellular automaton on a 3x3 Moore neighborhood in order to grow and reproduce a pattern. \n",
    "\n",
    "While the blog above uses emojis, we'll query the Wolfram Knowledgebase and use first-generation Pokemon characters instead.\n",
    "At the end, we'll end up with cellular automata models of the sort:\n",
    "\n",
    "![charizard](https://raw.githubusercontent.com/gvarnavi/generative-art-iap/master/01.27-Thursday/gifs/charizard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6bf3e4",
   "metadata": {},
   "source": [
    "## Target Images\n",
    "\n",
    "Like we said - we'll use the Generation I pokemon as our target images. We set some parameters, and query the Knowledgebase for standardized images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af2fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "numChannels = 16;\n",
    "prepaddedWidth = prepaddedHeight = 40;\n",
    "padding = 8;\n",
    "width = height = prepaddedWidth + 2 padding;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4113125",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize[img_Image] := Block[{width, height}, ImagePad[\n",
    "   ImageResize[ImagePad[img,\n",
    "     {width, height} = \n",
    "      ImageDimensions[img]; {Table[Round[1/2 Ramp[height - width]], \n",
    "       2], Table[Round[1/2 Ramp[width - height]], 2]}, \n",
    "     \"Fixed\"], {prepaddedWidth, prepaddedHeight}], padding]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4934e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generationOnePokemonClass = FilteredEntityClass[\"Pokemon\", EntityFunction[p, p[\"Generation\"] == Entity[\"PokemonGeneration\", \"GenerationI\"]]];\n",
    "EntityValue[generationOnePokemonClass = ComplementedEntityClass[generationOnePokemonClass, {Entity[\"Pokemon\", \"Pokedex0133:PartnerEevee\"], Entity[\"Pokemon\", \"Pokedex0025:PartnerPikachu\"]}], \"EntityCount\"]\n",
    "generationOnePokemonNames = EntityList[generationOnePokemonClass];\n",
    "generationOnePokemonImgs = standardize /@ EntityValue[generationOnePokemonClass, EntityProperty[\"Pokemon\", \"Image\"]];\n",
    "ImageCollage[generationOnePokemonImgs, Background -> None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e58493d",
   "metadata": {},
   "source": [
    "## Cell State\n",
    "\n",
    "- We'll represent each cell (or pixel) by 16 channels\n",
    "  - The first four are 'visible' and represent the RGBa values\n",
    "  - Layers 5-16 are 'hidden' layers and allow the neural net to learn our self-assembly or growth pattern\n",
    "- The Opacity a channel further encodes our cell's state\n",
    "  - If a > 0.1, the cell is considered 'mature'\n",
    "  - If a < 0.1, but at-least one of the cell's 3x3 neighbors is 'mature', the cell is considered 'growing'\n",
    "  - If none of the cell's 3x3 neighbors are 'mature', the cell is considered 'dead'\n",
    "  \n",
    "Note the pokemon images we have use the same scheme (i.e. use a transparent background or a channel for 'whitespace'). For visualization purposes we can code this using our Moore neighborhood function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  Moore[func__, lat_] :=   \n",
    "     MapThread[func, \n",
    "       Map[RotateRight[lat, #] &, \n",
    "         {{0, 0}, {1, 0}, {0, -1}, {-1, 0}, \n",
    "           {0, 1}, {1,  -1}, {-1, -1}, {-1, 1}, \n",
    "           {1, 1}}], 2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cellStateVisualization[self_, east_, south_, west_, north_, \n",
    "  southEast_, southWest_, northWest_, northEast_] := Which[\n",
    "  self > 0.1, 2,\n",
    "  Or @@ Thread[{east, south, west, north, southEast, southWest, \n",
    "      northWest, northEast} > 0.1], 1,\n",
    "  True, 0\n",
    "  ]\n",
    "  \n",
    "ImageCollage[\n",
    " ArrayPlot[\n",
    "    Moore[cellStateVisualization, Map[Last, ImageData[#], {2}]], \n",
    "    Frame -> False] & /@ RandomSample[generationOnePokemonImgs, 12], \n",
    " Background -> None, ImageSize -> 600]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b80b9",
   "metadata": {},
   "source": [
    "## Neural Network (Single Update Step)\n",
    "\n",
    "We follow the prescription from the blog above quite closely.\n",
    "A single update step for our cellular automaton can be summarized graphically by:\n",
    "\n",
    "![nn](https://raw.githubusercontent.com/gvarnavi/generative-art-iap/master/01.27-Thursday/gifs/neural-network-CAs.png)\n",
    "\n",
    "which consists of the following steps:\n",
    "\n",
    "1. Perception\n",
    "  - We use three fixed kernels to allow the cell  to perceive its 'local' environment\n",
    "  - In particular we use Sobel kernels to encode the gradient\n",
    "  - and concatenate them with the cell's identity to give a 16*3 = 48 dimensional vector (for each cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel[\"sobel-x\"]={{-1,0,1},{-2,0,2},{-1,0,1}};\n",
    "kernel[\"sobel-y\"]=Transpose[kernel[\"sobel-x\"]];\n",
    "kernel[\"identity\"]=BoxMatrix[0,{3,3}];\n",
    "\n",
    "conv[\"sobel-x\"]=ConvolutionLayer[\"Weights\"->Table[KroneckerDelta[i,j]Reverse[kernel[\"sobel-x\"],{1,2}]+ConstantArray[0,{3,3}],{i,numChannels},{j,numChannels}],\"Biases\"->None,PaddingSize->1,\"Interleaving\"->True,LearningRateMultipliers->None];\n",
    "conv[\"sobel-y\"]=ConvolutionLayer[\"Weights\"->Table[KroneckerDelta[i,j]Reverse[kernel[\"sobel-y\"],{1,2}]+ConstantArray[0,{3,3}],{i,numChannels},{j,numChannels}],\"Biases\"->None,PaddingSize->1,\"Interleaving\"->True,LearningRateMultipliers->None];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[\"perception\"] = \n",
    " NetGraph[<|\"sobel-x\" -> conv[\"sobel-x\"], \n",
    "   \"sobel-y\" -> conv[\"sobel-y\"], \n",
    "   \"catenate\" -> CatenateLayer[3]|>, {NetPort[\"Input\"] -> \"sobel-x\", \n",
    "   NetPort[\"Input\"] -> \n",
    "    \"sobel-y\", {NetPort[\"Input\"], \"sobel-x\", \"sobel-y\"} -> \n",
    "    \"catenate\" -> NetPort[\"percepted\"]}, \n",
    "  \"Input\" -> {width, height, numChannels}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb642b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[\"perception\"][RandomReal[{0, 1}, {56, 56, 16}]] // Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3eb376",
   "metadata": {},
   "source": [
    "Note this part is fully-initialized (i.e. has no trainable parameters). Also note that we use Interleaving for convenience (and keep the channels as the last tensor dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596c9a5",
   "metadata": {},
   "source": [
    " 2. Update Rule\n",
    "  - For each cell, we then apply a dense feed-forward network to go from this perception vector back to a state vector\n",
    "  - In particular we use a 128 Dense layer, followed by a ReLU activation layer, and a final 16 Dense layer\n",
    "  - We then use  NetMapOperator twice to apply this at each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[\"update\"] = \n",
    " NetMapOperator[\n",
    "  NetMapOperator[\n",
    "   NetChain[{LinearLayer[numChannels 8], Ramp, \n",
    "     LinearLayer[numChannels, \"Weights\" -> 0]}, \n",
    "    \"Input\" -> numChannels 3]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12580778",
   "metadata": {},
   "source": [
    "Note we've initialized the weights of the last dense layer to 0. This is to ensure 'do-nothing' initial behavior and keep the gradients low. Also note this is the only layer with trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860da0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "NetInitialize[net[\"update\"]][net[\"perception\"][RandomReal[{0, 1}, {56, 56, 16}]]]//Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd839ba6",
   "metadata": {},
   "source": [
    "3. Dropout layer (per cell)\n",
    " - Next, we apply a per cell Dropout layer to simulate the lack of a global clock in self-organizing systems (see blog post for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095601bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[\"cell-dropout\"] = \n",
    " NetChain[{NetArrayLayer[\n",
    "    \"Array\" -> ConstantArray[1/2, {width, height}], \n",
    "    LearningRateMultipliers -> 0], DropoutLayer[], \n",
    "   ReplicateLayer[numChannels, 3]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e25597",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tally[Flatten[net[\"cell-dropout\"][]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3daac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tally[Flatten[net[\"cell-dropout\"][NetEvaluationMode -> \"Train\"]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0525eb",
   "metadata": {},
   "source": [
    "Note we used a constant array of 1/2, since \n",
    "DropoutLayer sets the input elements to zero with probability p during training, multiplying the remainder by 1/(1-p), \n",
    "and we used the default  p=1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee34e0",
   "metadata": {},
   "source": [
    "4. Living cell mask\n",
    "  - And finally, we apply a pre-update and post-update living cell mask\n",
    "    - where we've defined 'living' as either 'mature' or 'growing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[\"living\"] = \n",
    " FunctionLayer[\n",
    "  PartLayer[{All, All, 1}][\n",
    "     PoolingLayer[3, \"PaddingSize\" -> 1, \n",
    "       \"Input\" -> {width, height, 1}, Interleaving -> True][\n",
    "      PartLayer[{All, All, 4 ;; 4}][#]]] > 0.1 &]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4adca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image[net[\"living\"][ImageData[generationOnePokemonImgs[[25]]]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe0cb2d",
   "metadata": {},
   "source": [
    "Putting it all together, we have our single update cell net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0614d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[\"single-update\"] = \n",
    " NetGraph[<|\"perception\" -> net[\"perception\"], \n",
    "   \"update\" -> net[\"update\"], \"cell-dropout\" -> net[\"cell-dropout\"], \n",
    "   \"dot-plus\" -> FunctionLayer[Apply[#1 #2 + #3 &]], \n",
    "   \"pre-living\" -> net[\"living\"], \"post-living\" -> net[\"living\"], \n",
    "   \"times\" -> FunctionLayer[Apply[#1 #2 #3 &]]|>,\n",
    "  {NetPort[\"Input\"] -> \"pre-living\", \n",
    "   NetPort[\"Input\"] -> \n",
    "    \"perception\" -> \"update\", {\"cell-dropout\", \"update\", \n",
    "     NetPort[\"Input\"]} -> \n",
    "    \"dot-plus\" -> \"post-living\", {\"pre-living\", \"post-living\", \n",
    "     \"dot-plus\"} -> \"times\" -> NetPort[\"Output\"]}, \n",
    "  \"Input\" -> {width, height, numChannels}, \n",
    "  \"Output\" -> {width, height, numChannels}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb669f",
   "metadata": {},
   "source": [
    "## Nested Network\n",
    "\n",
    "We now wish to nest this network, using the same set of trainable parameters.\n",
    " NetNestOperator seems like a perfect fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[\"nested\"] = NetChain[{NetNestOperator[net[\"single-update\"], 64], PartLayer[{All, All, ;; 4}]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3439a60",
   "metadata": {},
   "source": [
    "Note that after we iterate n times, we drop the hidden layers (to allow direct loss comparison with the visible channels of the target image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771d64c",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We could now train this starting from a single seed against a single target Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "singleSeed = Normal@SparseArray[Table[{width/2, height/2, i} -> 1, {i, 4, numChannels}], {width,height, numChannels}];\n",
    "targetPokemon = ImageData[generationOnePokemonImgs[[6]]];\n",
    "\n",
    "(*\n",
    "NetTrain[net[\"nested\"],<|\"Input\"\\[Rule]{singleSeed},\"Target\"\\[Rule]\n",
    "{targetPokemon}|>,MaxTrainingRounds\\[Rule]10,RandomSeeding\\[Rule]1996]\n",
    "*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d779ab",
   "metadata": {},
   "source": [
    "### Parallel Nesting\n",
    "\n",
    "However, while this will likely learn the target image well - the fixed number of iterations will mean it'll have very little predictive power beyond the 64 number of iterations. I.e. the growth pattern will not be stable.  \n",
    "\n",
    "The authors of the blog post deal with this in two ways:\n",
    "- Using a Pool/Batch technique in their second 'persistent' experiment\n",
    "- Nesting for a random number of iterations between 64 and 96 iterations\n",
    "  - This should at-least ensure the system is stable for a limited number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c1531",
   "metadata": {},
   "source": [
    "We'll use a (much more expensive) variant of the second technique since I couldn't figure out how to get NetNestOperator take a random number efficiently. We'll nest three nets in parallel for 50, 64, and 78 iterations each and assign the maximum deviation as the current loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccce51a",
   "metadata": {},
   "source": [
    "We must ensure our parallel nested nets will share trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1df07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[\"single-shared\"] = NetInsertSharedArrays[net[\"single-update\"]];\n",
    "net[\"nested-parallel\"] = \n",
    " NetGraph[Association[\n",
    "   Join[(\"nest-\" <> ToString[#]) -> \n",
    "       NetGraph[<|\n",
    "         \"nested\" -> NetNestOperator[net[\"single-shared\"], #], \n",
    "         \"part\" -> PartLayer[{All, All, ;; 4}], \n",
    "         \"loss\" -> MeanSquaredLossLayer[]|>, {\"nested\" -> \n",
    "          \"part\" -> \"loss\"}] & /@ \n",
    "     Subdivide[50, 78, 2], {\"max\" -> \n",
    "      ThreadingLayer[Max]}]], {{\"nest-50\", \"nest-64\", \"nest-78\"} -> \n",
    "    \"max\" -> NetPort[\"Loss\"]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f919c",
   "metadata": {},
   "source": [
    "We train a net for each target image (~1hr per net on  my GPU) and obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(*\n",
    "NetTrain[net[\"nested-parallel\"],<|\"Input\"\\[Rule]{singleSeed},\"Target\"\\[Rule]{targetPokemon}|>,MaxTrainingRounds\\[Rule]25000,RandomSeeding\\[Rule]1996,TargetDevice\\[Rule]\"GPU\"]\n",
    "*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b8d1b5",
   "metadata": {},
   "source": [
    "![animation](https://raw.githubusercontent.com/gvarnavi/generative-art-iap/master/01.27-Thursday/gifs/differentiable-CAs.gif)\n",
    "\n",
    "As we can see - while some growth patterns are stable (like Charizard, Venusaur, and Arbok), others like Rattata, Caterpie, and Spearow are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae4334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wolfram Language 13",
   "language": "Wolfram Language",
   "name": "wolframlanguage13"
  },
  "language_info": {
   "codemirror_mode": "mathematica",
   "file_extension": ".m",
   "mimetype": "application/vnd.wolfram.m",
   "name": "Wolfram Language",
   "pygments_lexer": "mathematica",
   "version": "12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
